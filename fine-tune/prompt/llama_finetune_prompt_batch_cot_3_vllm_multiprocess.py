import re
import json
import time
import signal
import os
import torch
import pandas as pd
from prompt.single_expert.add_history import add_history
from prompt.single_expert.describe_single_node_testing_token_cot import *
import argparse
from torch.multiprocessing import Pool, set_start_method
from vllm import LLM, SamplingParams
import math

# 确保在多 GPU 场景中使用 spawn 方法来初始化多进程
try:
    set_start_method('spawn')
except RuntimeError:
    pass

parser = argparse.ArgumentParser(description='Distributed model inference script.')
parser.add_argument('--model_path', type=str, required=True)
parser.add_argument('--dataset_name', type=str, required=True)
parser.add_argument('--delay', type=int, required=True)
parser.add_argument('--strategy', type=str, required=True)
parser.add_argument('--test_dir', type=str, required=True)
parser.add_argument('--result_dir', type=str, required=True)
parser.add_argument('--index', type=int, required=True)
parser.add_argument('--multiple', type=int, required=True)
parser.add_argument('--gpu_count', type=int, required=True)

args = parser.parse_args()

print(f"model_path: {args.model_path}")
print(f"dataset_name: {args.dataset_name}")
print(f"delay: {args.delay}")
print(f"strategy: {args.strategy}")
print(f"test_dir: {args.test_dir}")
print(f"result_dir: {args.result_dir}")
print(f"index: {args.index}")
print(f"multiple: {args.multiple}")
print(f"gpu_count: {args.gpu_count}")


def initialize_model(gpu_id):
    from vllm import LLM
    import os
    os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
    print(f"[GPU {gpu_id}] Initializing model...")
    llm = LLM(
        model=args.model_path,
        max_model_len=4096
    )
    print(f"[GPU {gpu_id}] Model initialized successfully.")
    return llm
    

def get_output_label_confidence_vllm(llm, messages):
    # 清理 GPU 缓存（可选）
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()

    # 配置采样参数
    sampling_params = SamplingParams(
        max_tokens=4096,
        temperature=0.0,
        logprobs=5  # 获取 token 的 log probabilities
    )

    # 生成输出
    output = llm.generate(messages, sampling_params)

    # 提取生成的文本
    generated_text = output[0].outputs[0].text
    print(f"Generated Text: {generated_text}")

    # 检查 logprobs 是否存在
    if not output[0].outputs[0].logprobs:
        print("No logprobs generated by the model.")
        return generated_text, "0", 0.5

    # 获取 logprob 数据（token ID -> Logprob 对象）
    logprob_data = output[0].outputs[0].logprobs[0]
    print(f"Logprob Data: {logprob_data}")

    # 提取第一个 token 及其 logprob
    first_token_id, first_logprob_obj = next(iter(logprob_data.items()))  # 提取第一个条目

    # 获取解码后的 token 和 log probability
    decoded_token = first_logprob_obj.decoded_token.strip()
    logprob_value = first_logprob_obj.logprob

    # 将 logprob 转换为标准概率
    probability = math.exp(logprob_value)

    # 打印结果
    print(f"First Token: {decoded_token}")
    print(f"Log Probability: {logprob_value}")
    print(f"Probability: {probability:.4f}")

    # 返回生成的文本、首个 token 及其概率
    return generated_text, decoded_token, probability


# 超时处理函数
def timeout_handler(signum, frame):
    raise TimeoutError("Timeout occurred")

# 使用信号机制实现超时逻辑
def describe_node_with_timeout(node, add_histroy_graph, labels_test, timeout=300):
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(timeout)  # 设置超时时间
    try:
        result = describe_single_node_test(add_histroy_graph, labels_test, node)
        signal.alarm(0)  # 取消超时
        return result
    except TimeoutError:
        print(f"Timeout while processing node {node}.")
        return None, None, None
    except Exception as e:
        print(f"Error processing node {node}: {e}")
        return None, None, None
    

# 每个 GPU 执行推理任务
def process_node_on_gpu(gpu_id, nodes, add_histroy_graph, labels_test):
    llm = initialize_model(gpu_id)
    results = []

    for node in nodes:
        # print(f"Processing node on GPU {gpu_id}: {node}")
        test_node_detail, is_phishing, _ = describe_node_with_timeout(node, add_histroy_graph, labels_test)
        if test_node_detail == None:
            results.append((node, None, None, None, 0, 0.5))
        else:
            print(f"len test_node_detail: {len(test_node_detail)}")   
            messages = f"""
# Instruction  
You are a blockchain analyst specializing in identifying fraudulent behavior within the Ethereum network. Your task is to analyze the transaction data of a **Target Node** and classify it as either a **Phishing Node** or **Non-Phishing Node**. This is a **binary classification problem**, requiring you to return one of the following:
1: Phishing Node
0: Non-Phishing Node 

## Task Details:  
To accurately classify the node, perform a step-by-step analysis based on the following dimensions. Use the **Chain-of-Thought (CoT)** methodology to reason through your analysis, combining observations, logical inference, and conclusions:
1. **Node Transaction Activity**  
- **Observation**: Evaluate the node's in-degree and out-degree, focusing on its overall activity.  
- **Inference**: A high number of total external transactions suggests significant activity, which, combined with high centrality, could indicate risky behavior.  
- **Conclusion**: Determine if the node's transaction activity is abnormally high and whether its central role in the network raises concerns.  
2. **Transaction Amount and Distribution**  
- **Observation**: Examine the frequency and ratio of zero-value and small-value transactions for both incoming and outgoing activities.  
- **Inference**: High proportions of such transactions often signify obfuscation or deceptive intent, a pattern frequently observed in phishing nodes.  
- **Conclusion**: Determine if these patterns indicate deliberate efforts to confuse tracking mechanisms or simulate legitimate activity.  
3. **Balance Fluctuation**  
- **Observation**: Analyze the node's balance for sharp changes, including significant inflows and rapid outflows.  
- **Inference**: Large inflows followed by immediate and frequent outflows suggest fund distribution to obscure origins.  
- **Conclusion**: Evaluate whether the balance fluctuations are consistent with phishing node behavior. 
4. **Transaction Frequency and Timing**  
- **Observation**: Review the node's transaction timeline, particularly its activity following large transfers.  
- **Inference**: Frequent bursts of transactions after large inflows may indicate an attempt to distribute funds quickly and evade tracking.  
- **Conclusion**: Assess whether these patterns align with malicious fund movement strategies.  
5. **Interaction with Other Nodes**  
- **Observation**: Evaluate the node's connections with known phishing nodes, unknown entities, and its overall network interactions.  
- **Inference**: High outgoing interactions with phishing or unknown nodes raise significant red flags, as they are indicative of malicious intent.  
- **Conclusion**: Determine whether the node's connections suggest coordinated phishing activities.  
6. **Internal Transactions**  
- **Observation**: Investigate the volume and frequency of internal transactions.  
- **Inference**: Frequent low-value internal transactions often indicate self-circulation, a common tactic for obfuscating fund origins.  
- **Conclusion**: Assess whether the internal transaction patterns are consistent with attempts to hide financial flows.  

## Objective  
Accurately identifying phishing nodes is critical to protecting the financial ecosystem. Use the provided data to determine whether the node exhibits phishing behavior. Provide a concise analysis and end with a clear, definitive answer in this format:
### Output Label
(0 or 1)
### CoT Analysis
"To analyze the behavior of ..." or "To classify ..."

### Target Node Information  
{test_node_detail}

### Output Label
"""
            start_time_A = time.time()
            generated_text, first_char, confidence_score = get_output_label_confidence_vllm(llm, messages)
            if first_char == "0":
                label = 0
            elif first_char == "1":
                label = 1
            else:
                print(f"error first_char: {first_char}")
                label = 0
                confidence_score = 0
            end_time_A = time.time()
            execution_time_A = end_time_A - start_time_A
            print(f"[GPU {gpu_id}] Node {node} processed.")
            print(f"first_char: {first_char}, label: {label}, confidence_score: {confidence_score},  Predict time： {execution_time_A} s")
            results.append((node, is_phishing, label, generated_text, first_char, confidence_score))
    return results


def extract_first_non_repeating(output):
    # 使用正则表达式匹配第一个连续重复的子字符串
    match = re.match(r"(.*?)(\1+)?", output)
    if match:
        return match.group(1).strip()  # 返回第一个非重复子字符串并去除多余空格
    return output


# 分配节点到多个 GPU
def distribute_nodes_across_gpus(nodes, gpu_count):
    chunk_size = (len(nodes) + gpu_count - 1) // gpu_count
    print(f"[INFO] Distributed {len(nodes)} nodes across {gpu_count} GPUs. Chunk size: {chunk_size}")
    return [nodes[i * chunk_size: (i + 1) * chunk_size] for i in range(gpu_count)]

def main():
    agent = "single-expert"
    strategy = args.strategy
    dataset_name = args.dataset_name
    delay = args.delay
    index = str(args.index)
    multiple = args.multiple

    test_dir = f'{args.test_dir}/{dataset_name}/{multiple}/delay_{delay}'
    start_date_file_path = f'/root/data/{dataset_name}/start_times.json'
    train_start_date = load_json(start_date_file_path)["train"][index]
    test_start_date = load_json(start_date_file_path)["test"][index]
    graph_all_file_path = f'/root/data/{dataset_name}/{dataset_name}.pkl'
    graph_all = load_graph(graph_all_file_path)

    result_dir = f'{args.result_dir}/{agent}/{dataset_name}/{multiple}/delay_{delay}/{strategy}'  
    os.makedirs(result_dir, exist_ok=True)
    result_file_path = f'{result_dir}/{index}.xlsx'
    print(f'\n *** Get Started -- multiple: {multiple}, cluster_{index} ***\n')

    # 引入prompt
    prompts_file_path = "/root/prompt/prompt/SingleExpert.json"
    prompts = json.load(open(prompts_file_path, 'r'))
    message_task_A = prompts['task']
    messages_system_A = prompts['system']
    message_attention_A = prompts['attention']
    message_rule = prompts['rules']

    # Load 测试集 in 提示词
    print("================== add_histroy_graph for test ==================")
    graph_test_file_path = f'{test_dir}/{index}/test.pkl'
    labels_test_file_path = f'{test_dir}/{index}/test_labels.json'
    nodemap_test_file_path = f'{test_dir}/{index}/test_nodemap.json'
    graph_test = load_graph(graph_test_file_path)
    labels_test = load_json(labels_test_file_path)
    nodemap_test = load_json(nodemap_test_file_path)
    add_histroy_graph, labels_test = add_history(graph_all, graph_test, labels_test, nodemap_test, test_start_date,
                                                 delay)
    print(f"graph_test nodes: {len(graph_test)}")
    nodes = list(graph_test.nodes)

    # 分配节点给各 GPU
    node_chunks = distribute_nodes_across_gpus(nodes, args.gpu_count)

    # 多 GPU 推理
    with Pool(processes=args.gpu_count) as pool:
        all_results = pool.starmap(
            process_node_on_gpu,
            [(gpu_id, node_chunks[gpu_id], add_histroy_graph, labels_test)
             for gpu_id in range(args.gpu_count)]
        )

    # 汇总所有 GPU 的结果
    df_results = pd.DataFrame(columns=['index', 'node_address', 'is_phishing', 'label', 'confidence_score', 'response'])
    for gpu_results in all_results:
        for i, (node, is_phishing, label, generated_text, first_char, confidence_score) in enumerate(gpu_results):
            new_row = pd.DataFrame({
                'index': [i + 1],
                'node_address': [node],
                'is_phishing': [is_phishing],
                'label': [label],
                'confidence_score': [confidence_score],
                'response': [generated_text],
            })
            df_results = pd.concat([df_results, new_row], ignore_index=True)

    # 保存结果
    df_results.to_excel(result_file_path, index=False)
    print(f"Results saved to: {result_file_path}")

if __name__ == '__main__':
    main()
